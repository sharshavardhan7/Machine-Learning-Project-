{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Data loading and NLP preprocessing**"
      ],
      "metadata": {
        "id": "kaT-QtKbh6nt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZcXae9IJeeT",
        "outputId": "44a8b229-b2ad-4ee0-cb72-791ed017579f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 47692 samples.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import os\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab') # Download punkt_tab resource\n",
        "\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(\"/content/cyberbullying_tweets.csv\")\n",
        "    # Binary label: 0 for 'Not_cyberbullying', 1 for all other types\n",
        "    df['label'] = df['cyberbullying_type'].apply(lambda x: 0 if x == 'not_cyberbullying' else 1)\n",
        "    return df[['tweet_text', 'label']]\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercase, remove URLs/mentions/hashtags, punctuation\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = text.lower().strip()\n",
        "    # Tokenize, remove stopwords, lemmatize\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = load_data('/content/cyberbullying_tweets.csv')\n",
        "    df['processed_text'] = df['tweet_text'].apply(preprocess_text)\n",
        "    # Create the directory if it doesn't exist\n",
        "    output_dir = 'data'\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    df.to_csv(os.path.join(output_dir, 'processed_data.csv'), index=False)\n",
        "    print(f\"Processed {len(df)} samples.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train SVM, Random Forest, and BERT**"
      ],
      "metadata": {
        "id": "jGgKFeMGiEL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "class CyberbullyingDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.texts = texts.reset_index(drop=True)  # Reset index to ensure sequential access\n",
        "        self.labels = labels.reset_index(drop=True)  # Reset index for labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx]) if pd.notna(self.texts[idx]) else \"\"  # Handle NaN\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text, add_special_tokens=True, max_length=self.max_len,\n",
        "            return_token_type_ids=False, padding='max_length', truncation=True,\n",
        "            return_attention_mask=True, return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def train_classical_models(X_train, X_test, y_train, y_test):\n",
        "    # Fill NaN values with empty strings\n",
        "    X_train = X_train.fillna('')\n",
        "    X_test = X_test.fillna('')\n",
        "\n",
        "    vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    X_train_vec = vectorizer.fit_transform(X_train)\n",
        "    X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "    # SVM\n",
        "    svm = SVC(kernel='linear')\n",
        "    svm.fit(X_train_vec, y_train)\n",
        "    svm_pred = svm.predict(X_test_vec)\n",
        "    print(\"SVM Accuracy:\", accuracy_score(y_test, svm_pred))\n",
        "    print(classification_report(y_test, svm_pred))\n",
        "\n",
        "    # Random Forest\n",
        "    rf = RandomForestClassifier(n_estimators=100)\n",
        "    rf.fit(X_train_vec, y_train)\n",
        "    rf_pred = rf.predict(X_test_vec)\n",
        "    print(\"RF Accuracy:\", accuracy_score(y_test, rf_pred))\n",
        "    print(classification_report(y_test, rf_pred))\n",
        "\n",
        "    return svm, rf, vectorizer\n",
        "\n",
        "def train_bert(df):\n",
        "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "    # Reset indices after split to ensure sequential access\n",
        "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "        df['processed_text'], df['label'], test_size=0.2, random_state=42\n",
        "    )\n",
        "    train_texts = train_texts.reset_index(drop=True)\n",
        "    val_texts = val_texts.reset_index(drop=True)\n",
        "    train_labels = train_labels.reset_index(drop=True)\n",
        "    val_labels = val_labels.reset_index(drop=True)\n",
        "\n",
        "    train_dataset = CyberbullyingDataset(train_texts, train_labels, tokenizer)\n",
        "    val_dataset = CyberbullyingDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./bert_results',\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "        eval_strategy='epoch'  # Updated from evaluation_strategy\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    model.save_pretrained('./bert_model')\n",
        "    tokenizer.save_pretrained('./bert_model')\n",
        "    return model, tokenizer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = pd.read_csv('data/processed_data.csv')\n",
        "\n",
        "    # Check label distribution\n",
        "    print(\"Label distribution before split:\")\n",
        "    print(df['label'].value_counts())\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        df['processed_text'], df['label'], test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    print(\"\\nLabel distribution in y_train:\")\n",
        "    print(y_train.value_counts())\n",
        "    print(\"\\nLabel distribution in y_test:\")\n",
        "    print(y_test.value_counts())\n",
        "\n",
        "    print(\"Training Classical Models...\")\n",
        "    svm, rf, vectorizer = train_classical_models(X_train, X_test, y_train, y_test)\n",
        "\n",
        "    print(\"\\nTraining BERT...\")\n",
        "    bert_model, bert_tokenizer = train_bert(df)\n",
        "\n",
        "    # Save classical models\n",
        "    import joblib\n",
        "    output_dir = 'models'\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    joblib.dump(svm, os.path.join(output_dir, 'svm_model.pkl'))\n",
        "    joblib.dump(rf, os.path.join(output_dir, 'rf_model.pkl'))\n",
        "    joblib.dump(vectorizer, os.path.join(output_dir, 'tfidf_vectorizer.pkl'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 976
        },
        "id": "bkT7DB-6Q0ow",
        "outputId": "3be21cc8-3d30-482c-bbb9-9ca3c9f1951b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label distribution before split:\n",
            "label\n",
            "1    39747\n",
            "0     7945\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Label distribution in y_train:\n",
            "label\n",
            "1    31832\n",
            "0     6321\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Label distribution in y_test:\n",
            "label\n",
            "1    7915\n",
            "0    1624\n",
            "Name: count, dtype: int64\n",
            "Training Classical Models...\n",
            "SVM Accuracy: 0.8607820526260614\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.31      0.43      1624\n",
            "           1       0.87      0.97      0.92      7915\n",
            "\n",
            "    accuracy                           0.86      9539\n",
            "   macro avg       0.79      0.64      0.68      9539\n",
            "weighted avg       0.84      0.86      0.84      9539\n",
            "\n",
            "RF Accuracy: 0.8429604780375302\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.35      0.43      1624\n",
            "           1       0.88      0.94      0.91      7915\n",
            "\n",
            "    accuracy                           0.84      9539\n",
            "   macro avg       0.72      0.65      0.67      9539\n",
            "weighted avg       0.82      0.84      0.83      9539\n",
            "\n",
            "\n",
            "Training BERT...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1276' max='14310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 1276/14310 04:53 < 49:59, 4.34 it/s, Epoch 0.27/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='14310' max='14310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [14310/14310 1:08:35, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.338300</td>\n",
              "      <td>0.366820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.440600</td>\n",
              "      <td>0.448146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.330840</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple inference script**"
      ],
      "metadata": {
        "id": "EgfP1Ha-iKvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import os\n",
        "import re # Import re for preprocess_text function\n",
        "from nltk.corpus import stopwords # Import stopwords for preprocess_text function\n",
        "from nltk.tokenize import word_tokenize # Import word_tokenize for preprocess_text function\n",
        "from nltk.stem import WordNetLemmatizer # Import WordNetLemmatizer for preprocess_text function\n",
        "import nltk # Import nltk for preprocess_text function\n",
        "\n",
        "# Assuming preprocess_text is available from the previous cell\n",
        "def preprocess_text(text):\n",
        "    # Lowercase, remove URLs/mentions/hashtags, punctuation\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = text.lower().strip()\n",
        "    # Tokenize, remove stopwords, lemmatize\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def predict_classical(text, model_type='svm'):\n",
        "    # Check if models are loaded\n",
        "    if 'vectorizer' not in globals() or ('svm' not in globals() and 'rf' not in globals()):\n",
        "        print(\"Classical models not loaded. Please run the training cell first.\")\n",
        "        return None\n",
        "\n",
        "    processed_text = preprocess_text(text)\n",
        "    vec = vectorizer.transform([processed_text])\n",
        "    if model_type == 'svm':\n",
        "        if 'svm' in globals():\n",
        "            pred = svm.predict(vec)[0]\n",
        "            return 'Bullying' if pred == 1 else 'Non-Bullying'\n",
        "        else:\n",
        "            print(\"SVM model not loaded.\")\n",
        "            return None\n",
        "    else:\n",
        "        if 'rf' in globals():\n",
        "            pred = rf.predict(vec)[0]\n",
        "            return 'Bullying' if pred == 1 else 'Non-Bullying'\n",
        "        else:\n",
        "            print(\"Random Forest model not loaded.\")\n",
        "            return None\n",
        "\n",
        "\n",
        "def predict_bert(text):\n",
        "    # Check if BERT model is loaded\n",
        "    if 'bert_tokenizer' not in globals() or 'bert_model' not in globals():\n",
        "        print(\"BERT model not loaded. Please run the training cell first.\")\n",
        "        return None\n",
        "\n",
        "    inputs = bert_tokenizer.encode_plus(text, return_tensors='pt', max_length=128, truncation=True)\n",
        "    outputs = bert_model(**inputs)\n",
        "    pred = torch.argmax(outputs.logits).item()\n",
        "    return 'Bullying' if pred == 1 else 'Non-Bullying'\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Create the directory if it doesn't exist\n",
        "    output_dir = 'models'\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Check if model files exist before loading\n",
        "    svm_model_path = os.path.join(output_dir, 'svm_model.pkl')\n",
        "    rf_model_path = os.path.join(output_dir, 'rf_model.pkl')\n",
        "    vectorizer_path = os.path.join(output_dir, 'tfidf_vectorizer.pkl')\n",
        "    bert_model_path = './bert_model'\n",
        "\n",
        "    if os.path.exists(svm_model_path) and os.path.exists(rf_model_path) and os.path.exists(vectorizer_path) and os.path.exists(bert_model_path):\n",
        "        # Load classical models\n",
        "        svm = joblib.load(svm_model_path)\n",
        "        rf = joblib.load(rf_model_path)\n",
        "        vectorizer = joblib.load(vectorizer_path)\n",
        "\n",
        "        # Load BERT\n",
        "        bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_path)\n",
        "        bert_model = AutoModelForSequenceClassification.from_pretrained(bert_model_path)\n",
        "\n",
        "        sample_tweet = \"You are so ugly, go kill yourself!\"\n",
        "        print(\"SVM:\", predict_classical(sample_tweet, 'svm'))\n",
        "        print(\"RF:\", predict_classical(sample_tweet, 'rf'))\n",
        "        print(\"BERT:\", predict_bert(sample_tweet))\n",
        "    else:\n",
        "        print(\"Model files not found. Please run the training cell (UhYATZPFJ4fh) first to train and save the models.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Fe_mR3GK5mq",
        "outputId": "ca86630f-0d74-4986-c398-a085a4586777"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM: Bullying\n",
            "RF: Non-Bullying\n",
            "BERT: Bullying\n"
          ]
        }
      ]
    }
  ]
}